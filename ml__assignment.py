# -*- coding: utf-8 -*-
"""ML _Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-oQlE9ZOa2Yi24RKvl0GwBx4dAfNNqsV
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = pd.read_csv('/content/tips (data for regression problem).csv')

# Check the dataset structure
print(data.head())

# Check the column names
print("Columns in dataset:", data.columns)

# Update target and features based on the actual column name
target_column = 'total_bill'  # Change this to the actual column name if different
if target_column not in data.columns:
    raise ValueError(f"'{target_column}' column not found in the dataset. Please check column names.")

# Define the target and features
X = data.drop(target_column, axis=1)  # Features
y = data[target_column]  # Target variable (tip amount)

# One-Hot Encode categorical variables
data = pd.get_dummies(data, drop_first=True)

# Define the target and features
X = data.drop('total_bill', axis=1)  # Features
y = data['total_bill']  # Target variable (tip amount)

# Perform train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features for certain models (like SVR and KNN)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Dictionary to store models and results
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
    'SVR': SVR(kernel='linear'),
    'KNN': KNeighborsRegressor(n_neighbors=5)
}

results = {}

# Train and evaluate each model
for model_name, model in models.items():
    print(f"\nTraining {model_name}...")

    # Use scaled data for models that are sensitive to feature scaling
    if model_name in ['SVR', 'KNN']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    results[model_name] = {'MSE': mse, 'R^2': r2}

    print(f"{model_name} - MSE: {mse:.4f}, R^2: {r2:.4f}")

# Display all results
print("\nModel Performance:")
for model, metrics in results.items():
    print(f"{model} - MSE: {metrics['MSE']:.4f}, R^2: {metrics['R^2']:.4f}")

# Feature importance for tree-based models and regularization models
def display_feature_importance(model, model_name, feature_names):
    if hasattr(model, 'coef_'):
        print(f"\n{model_name} Coefficients:")
        coef = model.coef_
        for feature, importance in zip(feature_names, coef):
            print(f"{feature}: {importance:.4f}")
    elif hasattr(model, 'feature_importances_'):
        print(f"\n{model_name} Feature Importances:")
        importances = model.feature_importances_
        for feature, importance in zip(feature_names, importances):
            print(f"{feature}: {importance:.4f}")

# Display feature importance for Lasso, Ridge, Decision Tree, and Random Forest
display_feature_importance(models['Lasso Regression'], 'Lasso Regression', X.columns)
display_feature_importance(models['Ridge Regression'], 'Ridge Regression', X.columns)
display_feature_importance(models['Decision Tree'], 'Decision Tree', X.columns)
display_feature_importance(models['Random Forest'], 'Random Forest', X.columns)

# Function to evaluate models with multiple metrics
def evaluate_model(name, model, X_train, y_train, X_test, y_test, use_scaled=False):
    # Fit model
    if use_scaled:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

    # Calculate metrics
    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    # Print metrics
    print(f"{name} - MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R^2: {r2:.2f}")
    return model, mae, mse, rmse, r2

# Initialize and evaluate each model
results = []

print("Linear Regression:")
linear_model, *metrics = evaluate_model("Linear Regression", LinearRegression(), X_train, y_train, X_test, y_test)
results.append(("Linear Regression", *metrics))

print("\nRidge Regression:")
ridge_model, *metrics = evaluate_model("Ridge Regression", Ridge(alpha=1.0), X_train, y_train, X_test, y_test)
results.append(("Ridge Regression", *metrics))

print("\nLasso Regression:")
lasso_model, *metrics = evaluate_model("Lasso Regression", Lasso(alpha=0.1), X_train, y_train, X_test, y_test)
results.append(("Lasso Regression", *metrics))

print("\nDecision Tree Regression:")
tree_model, *metrics = evaluate_model("Decision Tree Regression", DecisionTreeRegressor(max_depth=5), X_train, y_train, X_test, y_test)
results.append(("Decision Tree Regression", *metrics))

print("\nRandom Forest Regression:")
forest_model, *metrics = evaluate_model("Random Forest Regression", RandomForestRegressor(n_estimators=100, random_state=42), X_train, y_train, X_test, y_test)
results.append(("Random Forest Regression", *metrics))

print("\nSupport Vector Regression (SVR):")
svr_model, *metrics = evaluate_model("Support Vector Regression", SVR(C=1.0, epsilon=0.1), X_train, y_train, X_test, y_test, use_scaled=True)
results.append(("SVR", *metrics))

print("\nK-Nearest Neighbors Regression (KNN):")
knn_model, *metrics = evaluate_model("KNN Regression", KNeighborsRegressor(n_neighbors=5), X_train, y_train, X_test, y_test, use_scaled=True)
results.append(("KNN", *metrics))

# Display summary of all results
results_df = pd.DataFrame(results, columns=["Model", "MAE", "MSE", "RMSE", "R^2"])
print("\nSummary of Model Performance:")
print(results_df)

import matplotlib.pyplot as plt
import seaborn as sns

# Set up the style for the plots
sns.set(style="whitegrid")

# Plot MAE, RMSE, and R^2 scores for each model
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Plot Mean Absolute Error (MAE)
sns.barplot(x="Model", y="MAE", data=results_df, ax=axes[0], palette="Blues_d")
axes[0].set_title("Mean Absolute Error (MAE)")
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha="right")

# Plot Root Mean Squared Error (RMSE)
sns.barplot(x="Model", y="RMSE", data=results_df, ax=axes[1], palette="Greens_d")
axes[1].set_title("Root Mean Squared Error (RMSE)")
axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha="right")

# Plot R^2 Score
sns.barplot(x="Model", y="R^2", data=results_df, ax=axes[2], palette="Oranges_d")
axes[2].set_title("R^2 Score (Coefficient of Determination)")
axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=45, ha="right")

# Adjust layout and show plot
plt.tight_layout()
plt.show()

# Feature Importance Analysis using Random Forest
import matplotlib.pyplot as plt
import seaborn as sns

# Feature importance from Random Forest
importance = forest_model.feature_importances_
feature_names = X.columns

# Create a DataFrame for visualization
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(12, 6))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Feature Importance for Tip Amount Prediction')
plt.xlabel('Importance Score')
plt.ylabel('Feature')
plt.show()

# Print important features
print("Top 5 Important Features:")
print(importance_df.head(5))

# Scatter plot for each numerical feature against the target variable
numerical_features = X.select_dtypes(include=['float64', 'int64']).columns

plt.figure(figsize=(15, 10))
for i, feature in enumerate(numerical_features):
    plt.subplot(3, 3, i+1)
    sns.scatterplot(x=data[feature], y=data[target_column])
    plt.title(f'Scatter Plot: {feature} vs {target_column}')
    plt.xlabel(feature)
    plt.ylabel(target_column)

plt.tight_layout()
plt.show()

# Pair plot for numerical features
sns.pairplot(data[numerical_features.tolist() + [target_column]], diag_kind='kde')
plt.suptitle('Pair Plot of Features', y=1.02)
plt.show()

# Correlation matrix
correlation_matrix = data.corr()

plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True, linewidths=.5)
plt.title('Correlation Matrix')
plt.show()

import numpy as np
import pandas as pd

# Check the data types of each column
print("Data Types:")
print(data.dtypes)

# Convert the entire DataFrame to a NumPy array
data_array = np.asarray(data)

# Check the dtype of the array
print("\nData Array:")
print(data_array)
print("\nData Array Dtype:", data_array.dtype)

# Identify any non-numeric columns
non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns.tolist()
print("\nNon-Numeric Columns:", non_numeric_columns)

# Convert categorical columns to numeric using one-hot encoding
data_encoded = pd.get_dummies(data, drop_first=True)

# Re-check data types after encoding
print("\nEncoded Data Types:")
print(data_encoded.dtypes)

# Now we can convert to NumPy array again
data_array_cleaned = np.asarray(data_encoded)

# Check the new dtype
print("\nCleaned Data Array Dtype:", data_array_cleaned.dtype)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Linear Regression model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Make predictions
y_pred = linear_model.predict(X_test)

# Calculate performance metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Linear Regression Model Performance:")
print(f"MAE: {mae:.2f}, MSE: {mse:.2f}, RMSE: {rmse:.2f}, R^2: {r2:.2f}")